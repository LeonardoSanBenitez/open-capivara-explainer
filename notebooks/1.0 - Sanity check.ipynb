{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my capivaras!\n"
     ]
    }
   ],
   "source": [
    "import libs\n",
    "libs.print_hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds with jokes\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many helicopters can a human eat in one sitting?\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TinyLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TinyLlama over LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(url='http://llm-server:8080/health')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'message': {'content': 'It is not specified in the given text if the helicopters referred to are helicopters or other types of rotary-wing aircraft, so we cannot make direct comparisons between the food intake of human and non-human helicopters. The text implies that the helicopters can be eaten in one sitting, but this is not stated explicitly.',\n",
       "    'role': 'assistant'}}],\n",
       " 'created': 1723028034,\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'chat.completion',\n",
       " 'usage': {'completion_tokens': 79, 'prompt_tokens': 63, 'total_tokens': 142},\n",
       " 'id': 'chatcmpl-5ZUBOmJ2lztl6Z0md9efejv1Vdh5VdnY'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    url='http://llm-server:8080/v1/chat/completions',\n",
    "    headers={'Content-Type': 'application/json'},\n",
    "    json={'messages': messages},\n",
    ")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"http://llm-server:8080/v1\",\n",
    "    api_key = \"not-needed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"There is no definitive answer to this question, as the specific requirements of the human being and the quantity of food consumed can vary. However, some estimates suggest that humans can consume between 25-50 helicopters per day, based on their dietary needs. It's always best to consult a nutritionist or dietitian for specific recommendations.\", refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"not-needed\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"message\":{\"content\":\"It is not specified in the given text if the helicopters ref'\n",
      "****************\n",
      "b'erred to are helicopters or other types of rotary-wing aircraft, so we cannot make direct comparisons between the food intake of'\n",
      "****************\n",
      "b' human and non-human helicopters. The text implies that the helicopters can be eaten in one sitting, but this is not stated expl'\n",
      "****************\n",
      "b'icitly.\",\"role\":\"assistant\"}}],\"created\":1723028034,\"model\":\"gpt-3.5-turbo-0613\",\"object\":\"chat.completion\",\"usage\":{\"completion'\n",
      "****************\n",
      "b'_tokens\":79,\"prompt_tokens\":63,\"total_tokens\":142},\"id\":\"chatcmpl-5ZUBOmJ2lztl6Z0md9efejv1Vdh5VdnY\"}'\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"not-needed\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    #print(chunk.choices[0].delta.content)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TinyLlama over LlamaCpp over proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://explainer-orchestrator-proxy:80/docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Hello World'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    url='http://orchestrator-proxy:80/openai/deployments/not-needed/completions',\n",
    "    headers={'Content-Type': 'application/json', 'api-key': 'not-needed'},\n",
    "    json={'messages': messages},\n",
    ")\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://llm-server:8080/v1\",\n",
    "    api_key = \"not-needed\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
