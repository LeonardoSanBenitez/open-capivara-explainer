{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd99a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libs.plugin_converter.semantic_kernel_v0_to_openai_function import generate_callables, generate_definitions, make_context\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "\n",
    "import os\n",
    "from libs.utils.connector_llm import *\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "from libs.plugin_orchestrator.implementation_tool import OrchestratorWithTool\n",
    "from libs.plugin_orchestrator.implementation_bare import OrchestratorBare\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50c4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='{\"thought\": \"I need to retrieve the capital of France.\", \"action_name\": \"PluginCapital_get_capital\", \"args\": {\"country\": \"France\"}}' role='assistant' tool_calls=None\n",
      "[2024-08-12 14:04:10 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-12 14:04:10 +0000] libs.libs.plugin_orchestrator INFO     At step 1 the function PluginCapital_get_capital was requested, with args = {'country': 'France'}\n",
      "[2024-08-12 14:04:10 +0000] libs.libs.plugin_orchestrator INFO     result: Executed function PluginCapital_get_capital and returned: paris\n",
      "As this was the step 1/2, next you should call 'answer' with all the results you have.\n",
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='{\"thought\": \"The task is complete, I have obtained the information about the capital of France.\", \"action_name\": \"answer\", \"args\": {\"text\": \"The capital of France is <strong>Paris</strong>.\"}}' role='assistant' tool_calls=None\n",
      "[2024-08-12 14:04:11 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-12 14:04:11 +0000] libs.libs.plugin_orchestrator INFO     At step 2 the function answer was requested, with args = {'text': 'The capital of France is <strong>Paris</strong>.'}\n",
      "answer='The capital of France is <strong>Paris</strong>.' citations=[Citation(id='aff3f080', document='Result of function PluginCapital_get_capital', content=\"Executed function PluginCapital_get_capital and returned: paris\\nAs this was the step 1/2, next you should call 'answer' with all the results you have.\")] visualizations=[]\n",
      "\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n",
      "The capital of France is <strong>Paris</strong>.\n"
     ]
    }
   ],
   "source": [
    "#llm = factory_create_connector_llm(\n",
    "#    provider='llama_cpp',\n",
    "#    modelname='not-needed',\n",
    "#    version='not-needed',\n",
    "#    credentials=CredentialsOpenAI(\n",
    "#        base_url='http://llm-server:8080/v1',\n",
    "#        api_key='not-needed',\n",
    "#    )\n",
    "#)\n",
    "\n",
    "llm = factory_create_connector_llm(\n",
    "    provider='azure_openai',\n",
    "    modelname=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\"),\n",
    "    version=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\").split('-')[-1],\n",
    "    credentials=CredentialsOpenAI(\n",
    "        base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    ),\n",
    "    hyperparameters={'tool_choice': 'none'}\n",
    ")\n",
    "\n",
    "orchestrator = OrchestratorBare(\n",
    "    connection = llm,\n",
    "    tool_definitions = openai_function_to_tool.generate_definitions(semantic_kernel_v0_to_openai_function.generate_definitions([(PluginCapital(), \"PluginCapital\")])),\n",
    "    tool_callables = generate_callables([(PluginCapital(), \"PluginCapital\")]),\n",
    "    token_limit_input = 1024,\n",
    "    token_limit_output = None,\n",
    "    max_steps_recommended = 2,\n",
    "    max_steps_allowed = 3,\n",
    "    prompt_app_system='You are an AI assistant whose goal is to answer the user question.',\n",
    "    prompt_app_user='What is the capital of france?',\n",
    ")\n",
    "\n",
    "r = await orchestrator.run()\n",
    "print(r)\n",
    "print('\\n\\n' + '-'*30 + '\\n\\n')\n",
    "assert type(r.answer) == str\n",
    "assert len(r.answer) > 4\n",
    "print(r.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216bfeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(role='system', content='# General instructions\\nYour goal is to answer the user question, and you can call functions (aka commands/actions) to achieve that.\\nYour decisions must always be made independently without seeking user assistance.\\nEvery action has a cost, so be smart and efficient. You\\'ll be rewarded if you answer correctly and quickly.\\nAim to complete the goal in the least number of steps (aka function calls). Whenever you are ready to answer, use the function \\'answer\\'.\\nDo not take more than 2 steps to complete a task (including the function \\'answer\\').\\nIf at step 1 you don\\'t have all necessary information, answer the best you can with the information at hand.\\nAlways try to answer with as much useful information as you can.\\n\\nYou are allowed to call the following function:\\n1. PluginCapital_get_capital: Returns the name of the capital of a country..\\n  Arguments: country (Name of the country. Type: string. Required: yes)\\n2. answer: Send response back to the user. Show all your results here, this is the only thing that the user will see. You won\\'t be able to call any other function after this one. Be sure to return a complete and clear answer, the user will not be able to see any other intermediate messages nor ask for more information. Never mention intermediate messages or results; if you want to mention something, include it here. Call this function only once, with everything you want to show to the user..\\n  Arguments: text (final textual response to be send to the user. Should use HTML syntax for formatting. Type: string. Required: yes)\\n\\nThe JSON necessary to call a function contains one key named \"thought\" (its value have type string, with one short sentence descring why you should take this action), one key named \"action_name\" (its value have type string, with the name of the function to call), and one key named \"args\" (its value have type object, with the arguments to pass to the function).\\nThe JSON keys \"thought\", \"action_name\" and \"args\" should be written in this order.\\nThe JSON keys \"thought\", \"action_name\" and \"args\" should always be present.\\nExample (just to demonstrate the syntax, this example function does not exist, do not call it): {\"thought\": \"The first step to answer the user question is to use my example function\", \"action_name\": \"my_example_function\", \"args\": {\"my_param\": \"something\"}}\\nRemember to call the function only once per message.\\nDo not answer with anything that is not a json, do not add any extra comment, follow exactly the syntax provided.\\nAlways use the tools, do not answer without the tools.\\nYou are only allowed to use the functions described above, do not call any other function.\\n\\n# Asistant-specific instructions\\nYou are an AI assistant whose goal is to answer the user question.'),\n",
       " ChatCompletionMessage(role='user', content='What is the capital of france?'),\n",
       " ChatCompletionMessage(role='system', content=\"This will be your step 1. Now call a function with the JSON syntax explained.\\nDo not do anything else, just call a function.\\nIf you have enough information to answer the question, call the 'answer' function and pass the answer as a parameter.\\nThis is very important: the argument for the 'answer' function is the only thing the user will see, all your results should be shown here.\\nCall only one function per message.\\nDo not call the same function with the same arguments more than once, their outputs are deterministic.Call a function with the JSON syntax.\"),\n",
       " ChatCompletionMessage(role='assistant', content='{\"thought\": \"I need to retrieve the capital of France.\", \"action_name\": \"PluginCapital_get_capital\", \"args\": {\"country\": \"France\"}}'),\n",
       " ChatCompletionMessage(role='system', content=\"At step 1 the function PluginCapital_get_capital was requested, with args = {'country': 'France'}\"),\n",
       " ChatCompletionMessage(role='system', content=\"Executed function PluginCapital_get_capital and returned: paris\\nAs this was the step 1/2, next you should call 'answer' with all the results you have.\"),\n",
       " ChatCompletionMessage(role='assistant', content='{\"thought\": \"The task is complete, I have obtained the information about the capital of France.\", \"action_name\": \"answer\", \"args\": {\"text\": \"The capital of France is <strong>Paris</strong>.\"}}'),\n",
       " ChatCompletionMessage(role='system', content=\"At step 2 the function answer was requested, with args = {'text': 'The capital of France is <strong>Paris</strong>.'}\"),\n",
       " ChatCompletionMessage(role='assistant', content='The capital of France is <strong>Paris</strong>.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator._full_message_history_for_debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
