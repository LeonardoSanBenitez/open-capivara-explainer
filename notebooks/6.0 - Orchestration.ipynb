{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd99a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libs.plugin_converter.semantic_kernel_v0_to_openai_function import generate_callables, generate_definitions, make_context\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "from libs.plugins.plugin_salesforce import PluginSalesforce\n",
    "from libs.plugins.plugin_sap import PluginSAP\n",
    "from libs.plugins.plugin_capivara import PluginCapivaraAlert\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "\n",
    "import os\n",
    "from libs.utils.connector_llm import *\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "from libs.plugin_orchestrator.implementation_tool import OrchestratorWithTool\n",
    "from libs.plugin_orchestrator.implementation_bare import OrchestratorBare\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58af37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_kernel_v0_to_openai_function.generate_callables([(PluginSAP(), \"PluginSAP\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50c4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> FULLY ACCUMULATED ANSWER content='' role='assistant' tool_calls=[OpenAIToolCallPart(id='call_mq4lb8E25XeVS1aS0GNMmhVT', type='function', function=OpenaiFunctionCallPart(arguments='{\"amount\":10,\"material_id\":\"gloves\"}', name='PluginSAP_create_purchase_order'))]\n",
      "[2024-08-24 12:38:24 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: native API feature\n",
      "[2024-08-24 12:38:24 +0000] libs.libs.plugin_orchestrator INFO     At step 1 the function PluginSAP_create_purchase_order was requested, with args = {'amount': 10, 'material_id': 'gloves'}\n",
      "[2024-08-24 12:38:24 +0000] libs.libs.plugin_orchestrator INFO     result: Executed function PluginSAP_create_purchase_order and returned: Order placed successfully\n",
      ">>>>>>>>>>>> RECEIVED CHUNK {\"\n",
      ">>>>>>>>>>>> CLEANED CHUNK \n",
      ">>>>>>>>>>>> RECEIVED CHUNK text\n",
      ">>>>>>>>>>>> CLEANED CHUNK \n",
      ">>>>>>>>>>>> RECEIVED CHUNK \":\"\n",
      ">>>>>>>>>>>> CLEANED CHUNK \n",
      ">>>>>>>>>>>> RECEIVED CHUNK I\n",
      ">>>>>>>>>>>> CLEANED CHUNK I\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  have\n",
      ">>>>>>>>>>>> CLEANED CHUNK  have\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  successfully\n",
      ">>>>>>>>>>>> CLEANED CHUNK  successfully\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  placed\n",
      ">>>>>>>>>>>> CLEANED CHUNK  placed\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  an\n",
      ">>>>>>>>>>>> CLEANED CHUNK  an\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  order\n",
      ">>>>>>>>>>>> CLEANED CHUNK  order\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  for\n",
      ">>>>>>>>>>>> CLEANED CHUNK  for\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  \n",
      ">>>>>>>>>>>> CLEANED CHUNK  \n",
      ">>>>>>>>>>>> RECEIVED CHUNK 10\n",
      ">>>>>>>>>>>> CLEANED CHUNK 10\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  gloves\n",
      ">>>>>>>>>>>> CLEANED CHUNK  gloves\n",
      ">>>>>>>>>>>> RECEIVED CHUNK .\n",
      ">>>>>>>>>>>> CLEANED CHUNK .\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  Is\n",
      ">>>>>>>>>>>> CLEANED CHUNK  Is\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  there\n",
      ">>>>>>>>>>>> CLEANED CHUNK  there\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  anything\n",
      ">>>>>>>>>>>> CLEANED CHUNK  anything\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  else\n",
      ">>>>>>>>>>>> CLEANED CHUNK  else\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  you\n",
      ">>>>>>>>>>>> CLEANED CHUNK  you\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  need\n",
      ">>>>>>>>>>>> CLEANED CHUNK  need\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  assistance\n",
      ">>>>>>>>>>>> CLEANED CHUNK  assistance\n",
      ">>>>>>>>>>>> RECEIVED CHUNK  with\n",
      ">>>>>>>>>>>> CLEANED CHUNK  with\n",
      ">>>>>>>>>>>> RECEIVED CHUNK ?\"\n",
      ">>>>>>>>>>>> CLEANED CHUNK ?\n",
      ">>>>>>>>>>>> RECEIVED CHUNK }\n",
      ">>>>>>>>>>>> CLEANED CHUNK \n",
      ">>>>>>>>>>>>>>> FULLY ACCUMULATED ANSWER content='' role='assistant' tool_calls=[OpenAIToolCallPart(id='call_g9MfMZH14hBtUSRJpVz3BCiF', type='function', function=OpenaiFunctionCallPart(arguments='{\"text\":\"I have successfully placed an order for 10 gloves. Is there anything else you need assistance with?\"}', name='answer'))]\n",
      "[2024-08-24 12:38:25 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: native API feature\n",
      "[2024-08-24 12:38:25 +0000] libs.libs.plugin_orchestrator INFO     At step 2 the function answer was requested, with args = {'text': 'I have successfully placed an order for 10 gloves. Is there anything else you need assistance with?'}\n"
     ]
    }
   ],
   "source": [
    "# Bare openai\n",
    "#llm = factory_create_connector_llm(\n",
    "#    provider='azure_openai',\n",
    "#    modelname=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\"),\n",
    "#    version=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\").split('-')[-1],\n",
    "#    credentials=CredentialsOpenAI(\n",
    "#        base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "#    ),\n",
    "#    hyperparameters={'tool_choice': 'none'}\n",
    "#)\n",
    "\n",
    "# Tool openai\n",
    "llm = factory_create_connector_llm(\n",
    "    provider='azure_openai',\n",
    "    modelname=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\"),\n",
    "    version=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\").split('-')[-1],\n",
    "    credentials=CredentialsOpenAI(\n",
    "        base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "selected_alert = {'id': 'dc2a',\n",
    " 'title': 'Defective product',\n",
    " 'description': 'I ordered a Handstückschlauch, but it came already broken',\n",
    " 'severity': 'Medium'}\n",
    "\n",
    "plugins = [\n",
    "    (PluginCapivaraAlert(capivara_base_url='http://capivara:80', alert_id=selected_alert['id']), \"PluginServiceNow\"),\n",
    "    (PluginSalesforce(), \"PluginSalesforce\"),\n",
    "    (PluginSAP(), \"PluginSAP\"),\n",
    "]\n",
    "orchestrator = OrchestratorWithTool(\n",
    "    connection = llm,\n",
    "    tool_definitions = openai_function_to_tool.generate_definitions(semantic_kernel_v0_to_openai_function.generate_definitions(plugins)),\n",
    "    tool_callables = generate_callables(plugins),\n",
    "    token_limit_input = 1024,\n",
    "    token_limit_output = None,\n",
    "    max_steps_recommended = 4,\n",
    "    max_steps_allowed = 5,\n",
    "    prompt_app_system=(\n",
    "        'You are an AI assistant whose goal is to help the use handle a support ticket.\\n'\n",
    "        'Here is some info about this ticket:\\n'\n",
    "        f'Title: {selected_alert[\"title\"]}\\n'\n",
    "        f'Description: {selected_alert[\"description\"]}\\n'\n",
    "        f'Severity: {selected_alert[\"severity\"]}\\n'\n",
    "        'Answer the user questions and help the user to resolve this ticket.\\n'\n",
    "    ),\n",
    "    #prompt_app_user='hi =D'  # chitchat (don't work well)\n",
    "    #prompt_app_user='What purchases have this customer done in the past?',  # SAP\n",
    "    prompt_app_user='place an order for 10 gloves',  # SAP\n",
    "    #prompt_app_user='If the customer had purchased any glove in the past, please place an order for 100 of that exact glove model',  # SAP\n",
    "    #prompt_app_user='IF the customer purchased any hand desinfectant in the past, create an appointment for a sales call and describing we may want to do a follow up sales',  # SAP\n",
    "    #prompt_app_user='what is the full name of this customer?',  # Salesforce\n",
    "    #prompt_app_user='create an appointment asking an employee to visit the customer facility, and don\\'t forget to mention the customer address',  # Salesforce, SAP\n",
    "    #prompt_app_user='place an order for 10 gloves, then close the current ticket',  # SAP, capivara\n",
    "    #prompt_app_user='If the customer had purchased any glove in the past, please place an order for 100 of that exact glove model and close the current ticket',  # SAP, capivara\n",
    ")\n",
    "\n",
    "'''\n",
    "r = await orchestrator.run_async()\n",
    "#r = orchestrator.run()\n",
    "print(r)\n",
    "print('\\n\\n' + '-'*30 + '\\n\\n')\n",
    "assert type(r.answer) == str\n",
    "assert len(r.answer) > 4\n",
    "print(r.answer)\n",
    "''';\n",
    "\n",
    "#r_stream = orchestrator.run_stream_async()\n",
    "#async for chunk in r_stream:\n",
    "#    print(chunk, end='')\n",
    "\n",
    "r_stream = orchestrator.run_stream_async()\n",
    "chunks = []\n",
    "async for chunk in r_stream:\n",
    "    #print(chunk)\n",
    "    chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d18d87-a7a9-4b48-8648-3a1ee8df3dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ValidatedAnswerPart(answer=None, citations=None, visualizations=None, intermediate_results=[IntermediateResult(step=1, status_code=200, function_name='PluginSAP_create_purchase_order', function_arguments={'amount': 10, 'material_id': 'gloves'}, result='Order placed successfully', message='At step 1, executing PluginSAP_create_purchase_order', timestamp=1724503104, citation_id='1e83c45d')]),\n",
       " ValidatedAnswerPart(answer='', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='I', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' have', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' successfully', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' placed', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' an', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' order', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' for', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' ', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='10', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' gloves', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='.', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' Is', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' there', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' anything', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' else', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' you', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' need', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' assistance', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer=' with', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='?', citations=None, visualizations=None, intermediate_results=None),\n",
       " ValidatedAnswerPart(answer='', citations=None, visualizations=None, intermediate_results=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ff72a8-052c-4e6c-b19a-069255cb7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba2ccd-045a-48cc-bd6b-28bef5dd9a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62ca1571-f99c-4946-ba89-e0718a34b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.plugin_orchestrator.answer_validation import _process_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "320e1639-1e2f-4566-9eae-31f13048295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The customer had previously purchased the Untersuchungshandschuhe Nitril light lemon Gr. M gloves. I have placed an order for 100 of that exact glove model, and the current ticket has been successfully closed."
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from typing import AsyncGenerator, Dict\n",
    "\n",
    "async def mock_streaming() -> AsyncGenerator[str, None]:\n",
    "    chunks = [\n",
    "        '{\"',\n",
    "        'text',\n",
    "        '\":\"',\n",
    "        'The',\n",
    "        ' customer',\n",
    "        ' had',\n",
    "        ' previously',\n",
    "        ' purchased',\n",
    "        ' the',\n",
    "        ' Unters',\n",
    "        'uch',\n",
    "        'ung',\n",
    "        'sh',\n",
    "        'ands',\n",
    "        'chu',\n",
    "        'he',\n",
    "        ' N',\n",
    "        'itr',\n",
    "        'il',\n",
    "        ' light',\n",
    "        ' lemon',\n",
    "        ' Gr',\n",
    "        '.',\n",
    "        ' M',\n",
    "        ' gloves',\n",
    "        '.',\n",
    "        ' I',\n",
    "        ' have',\n",
    "        ' placed',\n",
    "        ' an',\n",
    "        ' order',\n",
    "        ' for',\n",
    "        ' ',\n",
    "        '100',\n",
    "        ' of',\n",
    "        ' that',\n",
    "        ' exact',\n",
    "        ' glove',\n",
    "        ' model',\n",
    "        ',',\n",
    "        ' and',\n",
    "        ' the',\n",
    "        ' current',\n",
    "        ' ticket',\n",
    "        ' has',\n",
    "        ' been',\n",
    "        ' successfully',\n",
    "        ' closed',\n",
    "        '.\"',\n",
    "        '}',\n",
    "    ]\n",
    "    for chunk in chunks:\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "\n",
    "persistent_state = {}\n",
    "processed_output = ''\n",
    "async for chunk in mock_streaming():\n",
    "    chunk_output, persistent_state = _process_chunk(chunk, persistent_state)\n",
    "    processed_output += chunk_output\n",
    "    print(chunk_output, end='')\n",
    "expected_output = (\n",
    "    \"The customer had previously purchased the Untersuchungshandschuhe \"\n",
    "    \"Nitril light lemon Gr. M gloves. I have placed an order for 100 of \"\n",
    "    \"that exact glove model, and the current ticket has been successfully closed.\"\n",
    ")\n",
    "assert processed_output == expected_output\n",
    "assert persistent_state['state'] == 'FINISHED'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6979e8a1-3819-44f5-ba49-d223228dcfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_state = {}\n",
    "\n",
    "chunk = '{\"text\":\"Hello World!\"}'\n",
    "processed_chunk, persistent_state = _process_chunk(chunk, persistent_state)\n",
    "\n",
    "#assert processed_chunk == \"Hello World!\"\n",
    "#assert persistent_state['state'] == 'FINISHED'\n",
    "processed_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e071f950-0233-480b-9f66-c3dc656be903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buffer': 'Hello World!\"}',\n",
       " 'state': 'READING_VALUE',\n",
       " 'key_index': 9,\n",
       " 'escape': False,\n",
       " 'unicode_escape': ''}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd444cc-4115-4c39-b4aa-83ded4068810",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLine 1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnLine 2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtTabbed\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m processed_chunk, persistent_state \u001b[38;5;241m=\u001b[39m _process_chunk(chunk, persistent_state)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m processed_chunk \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLine 1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLine 2\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTabbed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m persistent_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFINISHED\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "persistent_state = {}\n",
    "\n",
    "chunk = '{\"text\":\"Line 1\\\\nLine 2\\\\tTabbed\"}'\n",
    "processed_chunk, persistent_state = _process_chunk(chunk, persistent_state)\n",
    "\n",
    "assert processed_chunk == \"Line 1\\nLine 2\\tTabbed\"\n",
    "assert persistent_state['state'] == 'FINISHED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a08e986-89bc-4836-8048-758a9c66b266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f3adfa-7130-4033-af84-5d8933424909",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     chunk_output, persistent_state \u001b[38;5;241m=\u001b[39m _process_chunk(chunk, persistent_state)\n\u001b[1;32m      8\u001b[0m     processed_chunk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_output\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m processed_chunk \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnicode test: ☺\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m persistent_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFINISHED\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "persistent_state = {}\n",
    "\n",
    "chunks = ['{\"text\":\"Unicode ', 'test: \\\\u263A', '\"}']\n",
    "processed_chunk = ''\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_output, persistent_state = _process_chunk(chunk, persistent_state)\n",
    "    processed_chunk += chunk_output\n",
    "\n",
    "assert processed_chunk == \"Unicode test: ☺\"\n",
    "assert persistent_state['state'] == 'FINISHED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a968dd5b-1152-486b-9043-0e876fdef884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unicode test: 263A'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025392d-9d38-4b18-b13e-44cb9d1a2afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac817e7-eef2-499d-a318-53fa0d995b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_state = {}\n",
    "\n",
    "chunks = ['{\"text\":\"Hello ', 'World!', '\"}']\n",
    "processed_chunk = ''\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_output, persistent_state = _process_chunk(chunk, persistent_state)\n",
    "    processed_chunk += chunk_output\n",
    "\n",
    "assert processed_chunk == \"Hello World!\"\n",
    "assert persistent_state['state'] == 'FINISHED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8992045-a9d2-4321-93f5-b366212fa50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a7db2-458a-46de-a476-b1478fa0e051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae56c5-c627-4911-82d6-747fbdac42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "   persistent_state: Dict[str, str | int | bool] = {\n",
    "        'buffer': '',\n",
    "        'state': 'SEARCHING_KEY',\n",
    "        'key_index': 0,\n",
    "        'escape': False,\n",
    "        'unicode_escape': ''\n",
    "    }\n",
    "    \n",
    "    async for chunk in mock_streaming():\n",
    "        processed_chunk, persistent_state = process_chunk(chunk, persistent_state)\n",
    "        print(processed_chunk, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b54dab6c-9028-415a-b770-3ffcc8a596ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINISHED'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_state['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c955718e-07a3-42c3-9fee-bda3f08637ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b643ada1-8cb5-46f7-92bc-bb3b3bf291d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.mock_streaming()>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6d2cf-93d7-4bc3-8293-85f8631417d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d049aa-c2a3-4ce5-827c-e7942638b784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30eff432-9e49-4155-b2ad-86733452ac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessageResponse(content=None, role='assistant', tool_calls=[OpenAIToolCall(id='call_Hm4H0v1zGdqJ289MJPfS2Axz', type='function', function=OpenaiFunctionCall(arguments='{\"text\":\"Hello! How can I assist you today?\"}', name='answer'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await orchestrator._chat_step([ChatCompletionMessage(role='user', content='hi')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd89c5f-fc0d-4014-8bfa-d19969784042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id='call_DXc4WrFV45ygDdsBQy9X4Zzu', type='function', function=OpenaiFunctionCallPart(arguments='', name='answer'))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='{\"', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='text', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='\":\"', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='Hello', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' there', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='!', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' How', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' can', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' I', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' assist', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' you', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' today', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='?\"', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='}', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=None\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "response_stream = orchestrator._chat_step_stream([ChatCompletionMessage(role='user', content='hi')])\n",
    "async for chunk in response_stream:\n",
    "    print(chunk)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbadd5f-293e-444e-a4d3-53f530638d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id='call_w8nzNTwZ3W5FnfKCppWIoWs6', type='function', function=OpenaiFunctionCallPart(arguments='', name='answer'))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='{\"', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='text', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='\":\"', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='Hello', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='!', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' How', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' can', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' I', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' assist', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' you', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments=' today', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='?\"', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=[OpenAIToolCallPart(id=None, type=None, function=OpenaiFunctionCallPart(arguments='}', name=None))]\n",
      "--------------\n",
      "content='' role='assistant' tool_calls=None\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "self = orchestrator\n",
    "current_step_messages = [ChatCompletionMessage(role='user', content='hi')]\n",
    "\n",
    "\n",
    "chat_completion_stream = self.connection.chat_completion_stream_async(\n",
    "    messages=current_step_messages,\n",
    "    tool_definitions=self._tool_definitions_active,\n",
    ")\n",
    "async for chunk in chat_completion_stream:\n",
    "    if len(chunk.choices) > 0:\n",
    "        if chunk.choices[0].message is not None:\n",
    "            print(chunk.choices[0].message)\n",
    "            print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c3cbd-046b-4719-ade6-983e0bf79b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0bf70c-9eef-4875-b8c8-3dd74d36bbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bfeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Dataset generation\n",
    "orchestrator._full_message_history_for_debugging\n",
    "# This is the expected chat-over-text format:\n",
    "# <|system|> You are a chatbot who can help code!</s> <|user|> Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.</s> <|assistant|>\n",
    "# Before finetunning, we have to convert\n",
    "# this format is called \"Zephyr\", because it is used in the model HuggingFaceH4/zephyr-7b-alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113abc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d0506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117d4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a132a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca09ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d818444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Bare llamacpp\n",
    "llm = factory_create_connector_llm(\n",
    "    provider='llama_cpp',\n",
    "    modelname='not-needed',\n",
    "    version='not-needed',\n",
    "    credentials=CredentialsOpenAI(\n",
    "        base_url='http://llm-server:8080/v1',\n",
    "        api_key='not-needed',\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "plugins = [\n",
    "    (PluginCapital(), \"PluginCapital\"),\n",
    "]\n",
    "orchestrator = OrchestratorWithTool(\n",
    "    connection = llm,\n",
    "    tool_definitions = openai_function_to_tool.generate_definitions(semantic_kernel_v0_to_openai_function.generate_definitions(plugins)),\n",
    "    tool_callables = generate_callables(plugins),\n",
    "    token_limit_input = 1024,\n",
    "    token_limit_output = None,\n",
    "    max_steps_recommended = 2,\n",
    "    max_steps_allowed = 3,\n",
    "    prompt_app_system='You are an AI assistant whose goal is to answer the user question.',\n",
    "     prompt_app_user='What is the capital of france?',\n",
    ")\n",
    "\n",
    "r = await orchestrator.run_async()\n",
    "print(r)\n",
    "print('\\n\\n' + '-'*30 + '\\n\\n')\n",
    "assert type(r.answer) == str\n",
    "assert len(r.answer) > 4\n",
    "print(r.answer)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c647506-3ccc-47c6-a17f-2c4dd90f6301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eee46b-9ef6-4ffb-ae4c-8aa237983e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13224931-4f2c-44cb-82d7-29053fc36f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
