{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd99a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libs.plugin_converter.semantic_kernel_v0_to_openai_function import generate_callables, generate_definitions, make_context\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "from libs.plugins.plugin_salesforce import PluginSalesforce\n",
    "from libs.plugins.plugin_sap import PluginSAP\n",
    "from libs.plugins.plugin_capivara import PluginCapivaraAlert\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "\n",
    "import time\n",
    "import os\n",
    "from libs.utils.connector_llm import *\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "from libs.plugin_orchestrator.implementation_tool import OrchestratorWithTool\n",
    "from libs.plugin_orchestrator.implementation_bare import OrchestratorBare\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58af37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic_kernel_v0_to_openai_function.generate_callables([(PluginSAP(), \"PluginSAP\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c94c005-eb0e-4fef-9c12-1fede9d5f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b01535-ac75-4d2b-b1e1-9819d636902d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50c4e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='{\"thought\": \"I need to check the customer\\'s past purchase orders to see if they have bought any gloves before.\", \"action_name\": \"PluginSAP_get_purchase_orders\", \"args\": {}}' role='assistant' tool_calls=[]\n",
      "[2024-08-26 11:59:54 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-26 11:59:54 +0000] libs.libs.plugin_orchestrator INFO     At step 1 the function PluginSAP_get_purchase_orders was requested, with args = {}\n",
      "answer=None citations=None visualizations=None intermediate_results=[IntermediateResult(step=1, status_code=200, function_name='PluginSAP_get_purchase_orders', function_arguments={}, result='[{\"deliveryDate\": \"2021-01-01\", \"totalAmount\": 10000, \"items\": [{\"materialId\": \"IPS Natural Die Material ND1\", \"quantity\": 1}, {\"materialId\": \"Untersuchungshandschuhe Nitril light lemon Gr. M\", \"quantity\": 10}, {\"materialId\": \"Antiseptica r.f.u. H\\\\u00e4ndedesinfektion Flasche 1 Liter\", \"quantity\": 1}]}]', message='At step 1, executing PluginSAP_get_purchase_orders', timestamp=1724673594, citation_id='0c2846f8')]\n",
      "[2024-08-26 11:59:54 +0000] libs.libs.plugin_orchestrator INFO     result: Executed function PluginSAP_get_purchase_orders and returned: [{\"deliveryDate\": \"2021-01-01\", \"totalAmount\": 10000, \"items\": [{\"materialId\": \"IPS Natural Die Material ND1\", \"quantity\": 1}, {\"materialId\": \"Untersuchungshandschuhe Nitril light lemon Gr. M\", \"quantity\": 10}, {\"materialId\": \"Antiseptica r.f.u. H\\u00e4ndedesinfektion Flasche 1 Liter\", \"quantity\": 1}]}]\n",
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='{\"thought\": \"The customer has purchased Untersuchungshandschuhe Nitril light lemon Gr. M in the past. Placing an order for 100 of the exact same glove model.\", \"action_name\": \"PluginSAP_create_purchase_order\", \"args\": {\"amount\": 100, \"material_id\": \"Untersuchungshandschuhe Nitril light lemon Gr. M\"}}' role='assistant' tool_calls=[]\n",
      "[2024-08-26 11:59:55 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-26 11:59:55 +0000] libs.libs.plugin_orchestrator INFO     At step 2 the function PluginSAP_create_purchase_order was requested, with args = {'amount': 100, 'material_id': 'Untersuchungshandschuhe Nitril light lemon Gr. M'}\n",
      "answer=None citations=None visualizations=None intermediate_results=[IntermediateResult(step=2, status_code=200, function_name='PluginSAP_create_purchase_order', function_arguments={'amount': 100, 'material_id': 'Untersuchungshandschuhe Nitril light lemon Gr. M'}, result='Order placed successfully', message='At step 2, executing PluginSAP_create_purchase_order', timestamp=1724673595, citation_id='6cc23b9f')]\n",
      "[2024-08-26 11:59:55 +0000] libs.libs.plugin_orchestrator INFO     result: Executed function PluginSAP_create_purchase_order and returned: Order placed successfully\n",
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='{\"thought\": \"The order for 100 units of the glove model has been successfully placed. I will now close the current ticket.\", \"action_name\": \"PluginServiceNow_close_ticket\", \"args\": {}}' role='assistant' tool_calls=[]\n",
      "[2024-08-26 11:59:56 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-26 11:59:56 +0000] libs.libs.plugin_orchestrator INFO     At step 3 the function PluginServiceNow_close_ticket was requested, with args = {}\n",
      "answer=None citations=None visualizations=None intermediate_results=[IntermediateResult(step=3, status_code=200, function_name='PluginServiceNow_close_ticket', function_arguments={}, result='Ticket closed successfully', message='At step 3, executing PluginServiceNow_close_ticket', timestamp=1724673596, citation_id='ee6bf08a')]\n",
      "[2024-08-26 11:59:56 +0000] libs.libs.plugin_orchestrator INFO     result: Executed function PluginServiceNow_close_ticket and returned: Ticket closed successfully\n",
      "As this was the step 3/4, next you should call 'answer' with all the results you have.\n",
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='{\"thought\": \"The ticket has been successfully closed and the order for 100 gloves has been placed as per the customer\\'s request.\", \"action_name\": \"answer\", \"args\": {\"text\": \"The ticket has been successfully closed and the order for 100 gloves has been placed as per the customer\\'s request.\"}}' role='assistant' tool_calls=[]\n",
      "[2024-08-26 11:59:57 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-26 11:59:57 +0000] libs.libs.plugin_orchestrator INFO     At step 4 the function answer was requested, with args = {'text': \"The ticket has been successfully closed and the order for 100 gloves has been placed as per the customer's request.\"}\n"
     ]
    }
   ],
   "source": [
    "# Bare openai\n",
    "llm = factory_create_connector_llm(\n",
    "    provider='azure_openai',\n",
    "    modelname=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\"),\n",
    "    version=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\").split('-')[-1],\n",
    "    credentials=CredentialsOpenAI(\n",
    "        base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    ),\n",
    "    hyperparameters={'max_tokens': 2048, 'tool_choice': 'none'}\n",
    ")\n",
    "\n",
    "# Tool openai\n",
    "#llm = factory_create_connector_llm(\n",
    "#    provider='azure_openai',\n",
    "#    modelname=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\"),\n",
    "#    version=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_CHAT\").split('-')[-1],\n",
    "#    credentials=CredentialsOpenAI(\n",
    "#        base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "#        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "#    ),\n",
    "#)\n",
    "\n",
    "\n",
    "selected_alert = {'id': 'dc2a',\n",
    " 'title': 'Defective product',\n",
    " 'description': 'I ordered a HandstÃ¼ckschlauch, but it came already broken',\n",
    " 'severity': 'Medium'}\n",
    "\n",
    "plugins = [\n",
    "    (PluginCapivaraAlert(capivara_base_url='http://capivara:80', alert_id=selected_alert['id']), \"PluginServiceNow\"),\n",
    "    (PluginSalesforce(), \"PluginSalesforce\"),\n",
    "    (PluginSAP(), \"PluginSAP\"),\n",
    "]\n",
    "orchestrator = OrchestratorBare(\n",
    "    connection = llm,\n",
    "    tool_definitions = openai_function_to_tool.generate_definitions(semantic_kernel_v0_to_openai_function.generate_definitions(plugins)),\n",
    "    tool_callables = generate_callables(plugins),\n",
    "    token_limit_input = 2048,\n",
    "    token_limit_output = None,\n",
    "    max_steps_recommended = 4,\n",
    "    max_steps_allowed = 5,\n",
    "    prompt_app_system=(\n",
    "        'You are an AI assistant whose goal is to help the use handle a support ticket.\\n'\n",
    "        'Here is some info about this ticket:\\n'\n",
    "        f'Title: {selected_alert[\"title\"]}\\n'\n",
    "        f'Description: {selected_alert[\"description\"]}\\n'\n",
    "        f'Severity: {selected_alert[\"severity\"]}\\n'\n",
    "        'Answer the user questions and help the user to resolve this ticket.\\n'\n",
    "    ),\n",
    "    #prompt_app_user='hi =D'  # chitchat (don't work well)\n",
    "    #prompt_app_user='What purchases have this customer done in the past?',  # SAP\n",
    "    #prompt_app_user='place an order for 10 gloves',  # SAP\n",
    "    #prompt_app_user='If the customer had purchased any glove in the past, please place an order for 100 of that exact glove model',  # SAP\n",
    "    #prompt_app_user='IF the customer purchased any hand desinfectant in the past, create an appointment for a sales call and describing we may want to do a follow up sales',  # SAP\n",
    "    #prompt_app_user='what is the full name of this customer?',  # Salesforce\n",
    "    #prompt_app_user='create an appointment asking an employee to visit the customer facility, and don\\'t forget to mention the customer address',  # Salesforce, SAP\n",
    "    #prompt_app_user='close this ticket',  # Capivara\n",
    "    #prompt_app_user='place an order for 10 gloves, then close the current ticket',  # SAP, capivara\n",
    "    prompt_app_user='If the customer had purchased any glove in the past, please place an order for 100 of that exact glove model and close the current ticket',  # SAP, capivara\n",
    ")\n",
    "\n",
    "'''\n",
    "r = await orchestrator.run_async()\n",
    "#r = orchestrator.run()\n",
    "print(r)\n",
    "print('\\n\\n' + '-'*30 + '\\n\\n')\n",
    "assert type(r.answer) == str\n",
    "assert len(r.answer) > 4\n",
    "print(r.answer)\n",
    "''';\n",
    "\n",
    "#'''\n",
    "r_stream = orchestrator.run_stream_async()\n",
    "chunks = []\n",
    "async for chunk in r_stream:\n",
    "    #print(chunk.answer, '\\t\\t| ', len(chunk.intermediate_results or []))\n",
    "    print(chunk)\n",
    "    chunks.append(chunk)\n",
    "    #time.sleep(0.1)\n",
    "#''';\n",
    "\n",
    "'''\n",
    "r_stream = orchestrator.run_stream()\n",
    "chunks = []\n",
    "for chunk in r_stream:\n",
    "    #print(chunk)\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.answer, '\\t\\t| ', len(chunk.intermediate_results or []))\n",
    "    time.sleep(0.1)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216bfeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Dataset generation\n",
    "#orchestrator._full_message_history_for_debugging\n",
    "# This is the expected chat-over-text format:\n",
    "# <|system|> You are a chatbot who can help code!</s> <|user|> Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI.</s> <|assistant|>\n",
    "# Before finetunning, we have to convert\n",
    "# this format is called \"Zephyr\", because it is used in the model HuggingFaceH4/zephyr-7b-alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113abc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d0506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117d4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a132a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca09ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3096b93e-18a0-4959-b340-321d92b140b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libs.plugin_converter.semantic_kernel_v0_to_openai_function import generate_callables, generate_definitions, make_context\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "from libs.plugins.plugin_salesforce import PluginSalesforce\n",
    "from libs.plugins.plugin_sap import PluginSAP\n",
    "from libs.plugins.plugin_capivara import PluginCapivaraAlert\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "\n",
    "import time\n",
    "import os\n",
    "from libs.utils.connector_llm import *\n",
    "from libs.plugins.plugin_capital import PluginCapital\n",
    "import libs.plugin_converter.semantic_kernel_v0_to_openai_function as semantic_kernel_v0_to_openai_function\n",
    "import libs.plugin_converter.openai_function_to_tool as openai_function_to_tool\n",
    "from libs.plugin_orchestrator.implementation_tool import OrchestratorWithTool\n",
    "from libs.plugin_orchestrator.implementation_bare import OrchestratorBare\n",
    "from libs.plugin_orchestrator.implementation_bare_finetuned import OrchestratorBareFinetuned\n",
    "from libs.utils.json_resilient import *\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763d5d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content=\"```\\nconst { PluginCapital_get_capital } = require('plugin-capital');\\n\\nconst country = 'French Republic';\\n\\nconst capital = await PluginCapital_get_capital(country);\\n\\nconsole.log(capital); // Output: 'Paris'\\n```\\n\\nIn this example, the `PluginCapital_get_capital` function is called with the country as a parameter. The function is expected to return the name of the capital of the French Republic.\\n\\nStep 2: Answer the user's question\\nAfter executing the `PluginCapital_get_capital` function, the function's output is passed to the `answer\" role='assistant' tool_calls=[]\n",
      "[2024-08-27 06:48:54 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-27 06:48:54 +0000] libs.utils.json_resilient WARNING  json_loads_resilient: received jsonl with multiple elemnts, returning only the first valid element...\n",
      "[2024-08-27 06:48:54 +0000] libs.libs.plugin_orchestrator WARNING  Could not parse, error: {'Error': 'No valid json found within splits'}\n",
      "[2024-08-27 06:48:54 +0000] libs.libs.plugin_orchestrator WARNING  At step 1 the wrong syntax was used to call a function. The wrong syntax used was: ```\n",
      "const { PluginCapital_get_capital } = require('plugin-capital');\n",
      "\n",
      "const country = 'French Republic';\n",
      "\n",
      "const capital = await PluginCapital_get_capital(country);\n",
      "\n",
      "console.log(capital); // Output: 'Paris'\n",
      "```\n",
      "\n",
      "In this example, the `PluginCapital_get_capital` function is called with the country as a parameter. The function is expected to return the name of the capital of the French Republic.\n",
      "\n",
      "Step 2: Answer the user's question\n",
      "After executing the `PluginCapital_get_capital` function, the function's output is passed to the `answer\n",
      "THIS IS WHAT THE LLM IS TRYING TO DO:\n",
      "content='At step 2, you should call the `PluginCapital_get_capital` function with the country as the argument. This function expects the country name as an argument and returns the name of the capital of the French Republic.' role='assistant' tool_calls=[]\n",
      "[2024-08-27 06:49:04 +0000] libs.libs.plugin_orchestrator INFO     Function calling detected by: custom implementation\n",
      "[2024-08-27 06:49:04 +0000] libs.utils.json_resilient WARNING  json_loads_resilient: received jsonl with multiple elemnts, returning only the first valid element...\n",
      "[2024-08-27 06:49:04 +0000] libs.libs.plugin_orchestrator WARNING  Could not parse, error: {'Error': 'No valid json found within splits'}\n",
      "[2024-08-27 06:49:04 +0000] libs.libs.plugin_orchestrator WARNING  At step 2 the wrong syntax was used to call a function. The wrong syntax used was: At step 2, you should call the `PluginCapital_get_capital` function with the country as the argument. This function expects the country name as an argument and returns the name of the capital of the French Republic.\n",
      "[2024-08-27 06:49:04 +0000] libs.libs.plugin_orchestrator ERROR    Exceeded the maximum number of steps allowed: 2\n"
     ]
    },
    {
     "ename": "MaxStepsExceededError",
     "evalue": "Exceeded the maximum number of steps allowed: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMaxStepsExceededError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m r_stream \u001b[38;5;241m=\u001b[39m orchestrator\u001b[38;5;241m.\u001b[39mrun_stream_async()\n\u001b[1;32m     30\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r_stream:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39manswer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m| \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mintermediate_results \u001b[38;5;129;01mor\u001b[39;00m []))\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print(chunk)\u001b[39;00m\n",
      "File \u001b[0;32m/src/libs/plugin_orchestrator/implementation_tool.py:420\u001b[0m, in \u001b[0;36mOrchestratorWithTool.run_stream_async\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps_allowed:\n\u001b[1;32m    419\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExceeded the maximum number of steps allowed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps_allowed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxStepsExceededError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExceeded the maximum number of steps allowed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps_allowed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Send message to AI, get response\u001b[39;00m\n\u001b[1;32m    423\u001b[0m current_step_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_prompt_step()\n",
      "\u001b[0;31mMaxStepsExceededError\u001b[0m: Exceeded the maximum number of steps allowed: 2"
     ]
    }
   ],
   "source": [
    "# Bare llamacpp\n",
    "#'''\n",
    "llm = factory_create_connector_llm(\n",
    "    provider='llama_cpp',\n",
    "    modelname='not-needed',\n",
    "    version='not-needed',\n",
    "    credentials=CredentialsOpenAI(\n",
    "        base_url='http://llm-server:8080/v1',\n",
    "        api_key='not-needed',\n",
    "    ),\n",
    "    hyperparameters={'max_tokens': 150}\n",
    ")\n",
    "\n",
    "plugins = [\n",
    "    (PluginCapital(), \"PluginCapital\"),\n",
    "]\n",
    "orchestrator = OrchestratorBareFinetuned(\n",
    "    connection = llm,\n",
    "    tool_definitions = openai_function_to_tool.generate_definitions(semantic_kernel_v0_to_openai_function.generate_definitions(plugins)),\n",
    "    tool_callables = generate_callables(plugins),\n",
    "    token_limit_input = 1024,\n",
    "    token_limit_output = None,\n",
    "    max_steps_recommended = 2,\n",
    "    max_steps_allowed = 2,\n",
    "    prompt_app_system='You are an AI assistant whose goal is to answer the user question.',\n",
    "     prompt_app_user='What is the capital of france?',\n",
    ")\n",
    "\n",
    "r_stream = orchestrator.run_stream_async()\n",
    "chunks = []\n",
    "async for chunk in r_stream:\n",
    "    print(chunk.answer, '\\t\\t| ', len(chunk.intermediate_results or []))\n",
    "    #print(chunk)\n",
    "    chunks.append(chunk)\n",
    "    #time.sleep(0.1)\n",
    "#''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a80b-eea1-4c21-9880-bdd1f37f2e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c647506-3ccc-47c6-a17f-2c4dd90f6301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eefaae-0015-4772-8477-daf9c3f111d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
