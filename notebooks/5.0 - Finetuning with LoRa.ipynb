{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoSanBenitez/open-capivara-explainer/blob/main/notebooks/5.0%20-%20Finetuning%20with%20LoRa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh5BGSzcmeqY"
      },
      "source": [
        "Process:\n",
        "1. Set .env file with `HF_TOKEN=\"\"`\n",
        "1. Run the installation cell \"Training dependencies setup\"\n",
        "2. Restart\n",
        "3. Run all\n",
        "\n",
        "\n",
        "In the guff conversion it says it needs restart agian, but worked without (I kept the guff setup separately because of dependency conflicts)\n",
        "\n",
        "The downloaded models have to be put in the right folder afterwards, and maning adjusted\n",
        "\n",
        "# References\n",
        "\n",
        "[original tutorial](https://rocm.blogs.amd.com/artificial-intelligence/llama2-lora/README.html)\n",
        "\n",
        "[colorist tutorial](https://mychen76.medium.com/tinyllama-colorist-fine-tuned-with-color-dataset-8cd1cf7e5665)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7vmeoENPZQa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3737d3f8-0c25-4e65-f75b-101608217d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Training dependencies setup\n",
        "!pip install -q pandas peft==0.9.0 transformers==4.31.0 trl==0.4.7 bitsandbytes==0.43.0 accelerate scipy  tensorboardX python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "number_of_checkpoints = 4\n",
        "\n",
        "\n",
        "# mlabonne/guanaco-llama2-1k is a subset (1,000 samples) of the timdettmers/openassistant-guanaco\n",
        "# human-generated, human-annotated, assistant-style conversation corpus\n",
        "#dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "dataset_name = 'LeonardoBenitez/capivara-plugin-orchestration'\n",
        "dataset_config = 'plugins_capital_v1.0'\n",
        "\n",
        "\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "merged_model_repository = 'LeonardoBenitez/capivara-plugin-orchestration'\n",
        "merged_model_config = f\"{base_model_name.split('/')[1]}-LoRa-{dataset_config}\"  # Becomes a folder in the repository\n",
        "merged_model_name=\"merged_quantized.q8_0.gguf\"  # Becomes a file inside that folder\n",
        "\n",
        "training_params = {\n",
        "    'num_train_epochs': 10,  # tinyagent used 3 epochs\n",
        "    'learning_rate': 1.5e-4,  # colorist used 2e-4; tinyagent used 7e-5; original tutorial used 4e-5\n",
        "    'lr_scheduler_type': 'cosine',  # colorist used cosine\n",
        "}\n"
      ],
      "metadata": {
        "id": "mcHGZglnU1Ww"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "73fKrnc_U3xl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H57N-CnMdxFp",
        "outputId": "c8a856be-b83d-4526-a537-83dcdc3f49d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 18 11:15:51 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8              13W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import math\n",
        "import dotenv\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from huggingface_hub import HfApi\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from peft import get_peft_model\n",
        "from google.colab import files\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "assert type(os.getenv('HF_TOKEN')) == str\n",
        "assert len(os.getenv('HF_TOKEN')) > 0\n",
        "api = HfApi()\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exec_command(command: str) -> Tuple[bool, str]:\n",
        "    try:\n",
        "        output = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True, encoding=sys.getfilesystemencoding())\n",
        "        worked = True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        output = f'{e.returncode} {str(e.output)}'\n",
        "        worked = False\n",
        "    return worked, output"
      ],
      "metadata": {
        "id": "sGmqOoD5SNjg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWGduhd_s_Id",
        "outputId": "0fc2404e-382a-4048-9f03-1c2642170e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0 and are newly initialized: ['model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-21): 22 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
              "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"cuda\",\n",
        "    #load_in_8bit=True,\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n",
        "print(type(base_model))\n",
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ra6mnuEmC5u",
        "outputId": "1885cab4-bc9b-494c-949e-146bcf29d55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 1)\n",
            "{'text': '<|system|>\\nYou are allowed to call the following function:\\n1. PluginCapital_get_capital: Returns the name of the capital of a country..\\n  Arguments: country (Name of the country. Type: string. Required: yes)\\n2. answer: Send response back to the user. Show all your results here, this is the only thing that the user will see. You won\\'t be able to call any other function after this one. Be sure to return a complete and clear answer, the user will not be able to see any other intermediate messages nor ask for more information. Never mention intermediate messages or results; if you want to mention something, include it here. Call this function only once, with everything you want to show to the user..\\n  Arguments: text (final textual response to be send to the user. Should use HTML syntax for formatting. Type: string. Required: yes)\\n</s>\\n\\n<|user|>\\nI\\'m doing my school exam at http://www.baker-alexander.biz/, and it asks what is the capital of Nicaragua. What is the answer?</s>\\n\\n<|assistant|>\\n{\"thought\": \"The capital of nicaragua will be returned by PluginCapital_get_capital\", \"action_name\": \"PluginCapital_get_capital\", \"args\": {\"country\": \"nicaragua\"}}</s>\\n\\n'}\n"
          ]
        }
      ],
      "source": [
        "# Data set\n",
        "training_data = load_dataset(dataset_name, dataset_config, split='train')\n",
        "last_step = math.ceil(training_data.shape[0]/batch_size)\n",
        "save_steps = int(training_data.shape[0]/number_of_checkpoints)  # Save a checkpoint every {save_steps} steps\n",
        "print(training_data.shape)\n",
        "print(training_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqh2nTJSmllt",
        "outputId": "fe15ce16-4dd5-4b30-9937-88299c12477e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"/content/model_modified\"):\n",
        "  !rm -rf \"/content/model_modified\"\n",
        "os.mkdir(\"/content/model_modified\")\n",
        "\n",
        "# Training Params\n",
        "train_params = TrainingArguments(\n",
        "    output_dir=\"/content/model_modified\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=50,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    report_to=\"tensorboard\",\n",
        "    **training_params,\n",
        ")\n",
        "\n",
        "# LoRA Config\n",
        "peft_parameters = LoraConfig(\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.1,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(base_model, peft_parameters)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Trainer with LoRA configuration\n",
        "device = torch.device(\"cuda:0\")\n",
        "base_model= base_model.to(device)\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=training_data,\n",
        "    peft_config=peft_parameters,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=llama_tokenizer,\n",
        "    args=train_params,\n",
        "    max_seq_length=1024,  # colorist used this... but should not be the contex size, which is 2048?\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZiOWmIsGnQHL",
        "outputId": "500461d1-29c2-4e53-9221-644349537bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3310' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3310/40000 44:24 < 8:12:29, 1.24 it/s, Epoch 0.83/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.410100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.034300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.632400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.265300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.219300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.173400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.143600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.111900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.091500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.091800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.078900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.077100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.076400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.070600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.073900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.069800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.076400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.073100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.066000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.064500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.068400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.067900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.063000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.065100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.064100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.060400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.061100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.066100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.055400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.056100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.061300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.061600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.056800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.061000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.061400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.056400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.059200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.059300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.059900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.058800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.054700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.054600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.053300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.054700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.055200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.060000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.058300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.058400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.054600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.061600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.057200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Training\n",
        "# will run for {last_step} steps\n",
        "# May take a long time\n",
        "# 4k rows, batch size 2, 1 epoch: 26 minutes\n",
        "fine_tuning.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13nXsYIc0rHb"
      },
      "outputs": [],
      "source": [
        "# Download model\n",
        "# ~17MB\n",
        "'''\n",
        "assert os.path.exists(f\"/content/model_modified/checkpoint-{last_step}\")\n",
        "os.system(f\"zip -r model_modified_last_checkpoint.zip /content/model_modified/checkpoint-{last_step}\")\n",
        "assert os.path.exists(\"model_modified_last_checkpoint.zip\")\n",
        "files.download('model_modified_last_checkpoint.zip')\n",
        "''';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNUECwOY4cbh"
      },
      "source": [
        "# Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrQgcVWM4hvd"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "import os\n",
        "from google.colab import files\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "assert torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRxgBOU62E8e"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"cuda\",\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n",
        "\n",
        "# Merged\n",
        "inference_model = PeftModel.from_pretrained(base_model, f\"/content/model_modified/checkpoint-{last_step}\")\n",
        "device = torch.device(\"cuda:0\")\n",
        "inference_model= inference_model.to(device)\n",
        "print(type(inference_model))\n",
        "inference_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n5ogByZ108W"
      },
      "outputs": [],
      "source": [
        "batch = llama_tokenizer('''<|system|>\n",
        "You are allowed to call the following function:\n",
        "1. PluginCapital_get_capital: Returns the name of the capital of a country..\n",
        "  Arguments: country (Name of the country. Type: string. Required: yes)\n",
        "2. answer: Send response back to the user. Show all your results here, this is the only thing that the user will see. You won't be able to call any other function after this one. Be sure to return a complete and clear answer, the user will not be able to see any other intermediate messages nor ask for more information. Never mention intermediate messages or results; if you want to mention something, include it here. Call this function only once, with everything you want to show to the user..\n",
        "  Arguments: text (final textual response to be send to the user. Should use HTML syntax for formatting. Type: string. Required: yes)\n",
        "</s>\n",
        "\n",
        "<|user|>\n",
        "What is the capital of France?</s>\n",
        "\n",
        "<|assistant|>''', return_tensors=\"pt\")\n",
        "batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "output_tokens = inference_model.generate(\n",
        "    **batch,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    eos_token_id=llama_tokenizer.eos_token_id,\n",
        "    pad_token_id=llama_tokenizer.pad_token_id,\n",
        ")\n",
        "assert type(output_tokens) == torch.Tensor\n",
        "\n",
        "assert len(output_tokens) == 1\n",
        "assert len(output_tokens[0]) > 10\n",
        "output_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaKD1gdl33Za"
      },
      "outputs": [],
      "source": [
        "target_predicted = llama_tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(\"<|endcontext|>\")[0]\n",
        "assert type(target_predicted) == str\n",
        "print(target_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ7FD-4T5B87"
      },
      "source": [
        "# Quantization and GUFF conversion\n",
        "\n",
        "Conversion instructions: https://github.com/ggerganov/llama.cpp/discussions/2948\n",
        "\n",
        "Loading a GGUF model with python transformers: https://github.com/ggerganov/llama.cpp/discussions/2948"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_k8Sb4b5SaX"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "import locale\n",
        "locale.getpreferredencoding = \"UTF-8\"\n",
        "if os.path.exists(\"llama.cpp\"):\n",
        "  exec_command('rm -rf \"llama.cpp\"')\n",
        "os.mkdir(\"llama.cpp\")\n",
        "\n",
        "worked, output = exec_command('git clone \"https://github.com/ggerganov/llama.cpp.git\"')\n",
        "assert worked\n",
        "worked, output = exec_command('pip install -r llama.cpp/requirements.txt')\n",
        "assert worked\n",
        "#!python llama.cpp/convert_lora_to_gguf.py -h\n",
        "#!ls -l llama.cpp/convert*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the config\n",
        "with open('/content/config.json', 'w') as f:\n",
        "    f.write(json.dumps({\n",
        "        'batch_size': batch_size,\n",
        "        'number_of_checkpoints': number_of_checkpoints,\n",
        "        'dataset_name': dataset_name,\n",
        "        'dataset_config': dataset_config,\n",
        "        'base_model_name': base_model_name,\n",
        "        'merged_model_repository': merged_model_repository,\n",
        "        'merged_model_config': merged_model_config,\n",
        "        'merged_model_name': merged_model_name,\n",
        "        'training_params': training_params,\n",
        "    }))"
      ],
      "metadata": {
        "id": "Hjia2vGYGKsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9RkxjrxC_9z"
      },
      "outputs": [],
      "source": [
        "# Saving the lora weights + base model (1~2 GBs)\n",
        "# You can run the inference with both `inference_model` or `merged_model`\n",
        "# Since the merge is oly needed for saving, i'm doing it here\n",
        "merged_model = inference_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"/content/model_merged\", save_adapters=True, save_embedding_layers=True)\n",
        "llama_tokenizer.save_pretrained(\"/content/model_merged\")\n",
        "\n",
        "worked, output = exec_command('python llama.cpp/convert_hf_to_gguf.py \"/content/model_merged\" --outfile \"/content/model_modified_merged_quantized.gguf\" --outtype q8_0')\n",
        "assert worked\n",
        "assert os.path.exists(\"/content/model_modified_merged_quantized.gguf\")\n",
        "\n",
        "# Upload the resulting file to HuggingFace\n",
        "# To use this file with LlamaCPP: [\"--hf-repo\", merged_model_repository, \"--hf-file\", \"{merged_model_config}/{merged_model_name}\"\"]\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/model_modified_merged_quantized.gguf\",  # Path to the model file\n",
        "    path_in_repo=os.path.join(merged_model_config, merged_model_name),\n",
        "    repo_id=merged_model_repository,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/config.json\",\n",
        "    path_in_repo=os.path.join(merged_model_config, 'config.json'),\n",
        "    repo_id=merged_model_repository,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "# Or you can download the file directly\n",
        "# This finish executing before the model is fully downloaded, so wait...\n",
        "# To use this file with llamaCPP: command: [\"--model\", \"/models/model_modified_merged_quantized.gguf\"]\n",
        "'''\n",
        "files.download('/content/model_modified_merged_quantized.gguf')\n",
        "''';\n",
        "\n",
        "# Zipping reduce about 4% size, so whatever...\n",
        "'''\n",
        "!zip /content/model_modified_merged_quantized.gguf.zip /content/model_modified_merged_quantized.gguf\n",
        "!ls -lah /content/model_modified_merged_quantized.gguf.zip\n",
        "files.download('/content/model_modified_merged_quantized.gguf.zip')\n",
        "''';\n",
        "\n",
        "\n",
        "# Another option is saving to your personal drive\n",
        "# This is faster and more reliable, but you have to setup the drive connection\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "!cp \"/content/model_modified_merged_quantized.gguf\" \"/content/gdrive/MyDrive/PATH_IN_YOUR_DRIVE/model_modified_merged_quantized.gguf\"\n",
        "''';\n",
        "\n",
        "# Maybe `gdown` is also an option\n",
        "# https://stackoverflow.com/questions/49428332/how-to-download-large-files-like-weights-of-a-model-from-colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqPGXNTc8aaL"
      },
      "outputs": [],
      "source": [
        "# Saving base model\n",
        "'''\n",
        "from huggingface_hub import snapshot_download\n",
        "model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "snapshot_download(\n",
        "    repo_id=model_id,\n",
        "    local_dir=\"/content/model_base\",\n",
        "    local_dir_use_symlinks=False,\n",
        "    revision=\"main\",\n",
        "  )\n",
        "\n",
        "!python llama.cpp/convert_hf_to_gguf.py \"/content/model_base\" \\\n",
        "  --outfile \"/content/model_base.gguf\" \\\n",
        "  --outtype q8_0\n",
        "\n",
        "assert os.path.exists(\"/content/model_base.gguf\")\n",
        "files.download('/content/model_base.gguf')\n",
        "''';\n",
        "\n",
        "# Saving just the lora weights (a few MBs)\n",
        "'''\n",
        "!python llama.cpp/convert_lora_to_gguf.py \"/content/model_modified/checkpoint-1000\" \\\n",
        "  --base \"/content/model_base\" \\\n",
        "  --outfile \"/content/model_modified_quantized.gguf\" \\\n",
        "  --outtype q8_0\n",
        "\n",
        "assert os.path.exists(\"/content/model_modified_quantized.gguf\")\n",
        "files.download('/content/model_modified_quantized.gguf')\n",
        "''';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBgDOqTMJQDy"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}