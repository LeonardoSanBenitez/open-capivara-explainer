version: "3.5"
services:
  notebooks:
    container_name: explainer-notebooks
    build: 
      context: .
      dockerfile: notebooks/Dockerfile
    command: ["jupyter", "notebook", "--port=8888", "--no-browser", "--ip=0.0.0.0", "--allow-root", "--NotebookApp.token=${NOTEBOOK_TOKEN}"]
    volumes:
      - ./:/src
    env_file:
      - .env
    environment:
      - PYTHONPATH=/src
    ports:
       - 8888:8888

  llm-server:
    # Docs: https://github.com/ggerganov/llama.cpp/tree/master/examples/server
    # When downloading a new model, it takes a while and it doesn't show any progress in the terminal
    # You can check if the download has completed by checking the file `models/.cache/llama.cpp/<name>.downloadInProgress`
    container_name: explainer-llm-server
    image: ghcr.io/ggerganov/llama.cpp:server
    command: ["--hf-repo", "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF", "--hf-file", "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf", "--host", "0.0.0.0", "--port", "8080"]
    volumes:
      - ./models/.cache:/root/.cache
      - ./models:/models
    ports:
      - "8080:8080"

  llm-server-agent:
    container_name: explainer-llm-server-agent
    image: ghcr.io/ggerganov/llama.cpp:server
    #command: ["--lora-base", "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "--lora", "/models/model_modified_quantized.gguf", "--host", "0.0.0.0", "--port", "8080"]
    command: ["--hf-repo", "LeonardoBenitez/capivara-plugin-orchestration", "--hf-file", "TinyLlama-1.1B-Chat-v1.0-LoRa-plugins_capital_v1.0/merged_quantized.q8_0.gguf", "--host", "0.0.0.0", "--port", "8080"]
    #command: ["--model", "/models/model_modified_merged_quantized.gguf", "--host", "0.0.0.0", "--port", "8080"]
    volumes:
      - ./models/.cache:/root/.cache
      - ./models:/models
    ports:
      - "8081:8080"

  orchestrator-proxy:
    container_name: explainer-orchestrator-proxy
    build: 
      context: .
      dockerfile: ./services/orchestrator_proxy/Dockerfile
    command: ["flask", "--app=app/main.py", "run", "--host=0.0.0.0", "--port=80", "--reload"]
    volumes:
      - ./services/orchestrator_proxy/app:/src/app
      - ./libs:/src/libs
    env_file:
      - .env
    environment:
      - PYTHONPATH=/src
    ports:
      - 8000:80

  capivara:
    container_name: explainer-capivara
    build: 
      context: .
      dockerfile: ./services/capivara/Dockerfile
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "80", "--reload"]
    volumes:
      - ./services/capivara/app:/src/app
      - ./libs:/src/libs
    env_file:
      - .env
    environment:
      - PYTHONPATH=/src
    ports:
      - 8001:80

  frontend:
    container_name: explainer-frontend
    build: 
      context: .
      dockerfile: ./services/frontend/Dockerfile
    command: ["streamlit", "run", "app/main.py", "--server.port=80", "--server.address=0.0.0.0"]
    volumes:
      - ./services/frontend/app:/src/app
      - ./libs:/src/libs
    env_file:
      - .env
    environment:
      - PYTHONPATH=/src
    ports:
      - 8501:80